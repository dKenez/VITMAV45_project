{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "zVC90ljNEAnM"
      },
      "source": [
        "## Data preparation and preprocessing\n",
        "\n",
        "### Short introduction\n",
        "\n",
        "We are the Conloquor team, which means dialogue in latin. We will be developing a chatbot for our project this semester.\n",
        "\n",
        "Members:\n",
        "\n",
        "- Béres Bálint\n",
        "- Drexler Konrád\n",
        "- Drexler Kristóf\n",
        "\n",
        "### Data source\n",
        "\n",
        "We found a dataset on [reddit](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/)\n",
        "which includes all the reddit comments categorized by month. A user uploaded the entire dataset to google's\n",
        "bigquery platform, here's the [reddit](https://www.reddit.com/r/bigquery/comments/3cej2b/17_billion_reddit_comments_loaded_on_bigquery/)\n",
        "thread.\n",
        "\n",
        "### Downloading the data\n",
        "\n",
        "We chose to use comments from may, 2015 for our project. For this we ran the following SQL query on the bigquery platform.\n",
        "\n",
        "```SQL\n",
        "select *\n",
        "from `fh-bigquery.reddit_comments.2015_05`\n",
        "where subreddit like 'science'\n",
        "    or subreddit like 'politics'\n",
        "    or subreddit like 'gaming'\n",
        "    or subreddit like 'worldnews'\n",
        "    or subreddit like 'CasualConversation'\n",
        "    or subreddit like 'sports'\n",
        "```\n",
        "\n",
        "At first we downloaded all the comments made that month, but the resulting file was 5GB compressed.\n",
        "Therefore, we limited the source subreddits to **r/science**, **r/politics**, **r/gaming**,\n",
        "**r/worldnews**, **r/CasualConversation** and **r/sports**. This query still yielded 1.45 million\n",
        "comments to work with, but was a manageable size. We exported the resulting table to a json file;\n",
        " `data_2015_05.json`. This `.json` is available on my [google drive](https://drive.google.com/file/d/13n1ET0mppD6i-DjqyJIFjAiMQp6V7v6q/view?usp=sharing).\n",
        " In the future, the project will automatically download the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "B0tqJEd5EAnN"
      },
      "source": [
        "## Formatting the data for preprocessing\n",
        "\n",
        "The initial json file still had a lot of unnecessary columns and unusable rows. Using further SQL queries we\n",
        "trimmed and transformed the data to fit our needs. At the end of the process we were left with just short of\n",
        "480000 message-response pairs. This was done by by filtering out messages longer than 200 characters, [deleted] messages\n",
        "and hyperlink only messages to name a few."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "F2t9L57HEAnN"
      },
      "source": [
        "# pandasql is not in the google colab repertoir by default, it needs to be installed manually\n",
        "!pip install pandasql"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "IHW_hVhOEAnO"
      },
      "source": [
        "# import statements\n",
        "import pandas as pd\n",
        "import pandasql as ps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "znIvIiU4EAnO"
      },
      "source": [
        "# Create dataframe from json file\n",
        "raw_data_df = pd.read_json(r'data_2015_05.json', orient='records', lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "QvyRADdoEAnO"
      },
      "source": [
        "# Show top ten rows\n",
        "raw_data_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "wqIrLdkvEAnO"
      },
      "source": [
        "# Filter raw data:\n",
        "# select only rows which have a length less than 200, and the comment wasn't [deleted]\n",
        "sql_query = \" select body\" \\\n",
        "            \"       , name\" \\\n",
        "            \"       , link_id\" \\\n",
        "            \"       , parent_id\" \\\n",
        "            \"       , score\" \\\n",
        "            \" from raw_data_df\" \\\n",
        "            \" where length(body) < 200 and body <> '[deleted]'\"\n",
        "# Can only be saved as sdf since this is how pandas works.\n",
        "sdf = ps.sqldf(sql_query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "d1LSyYWdEAnP"
      },
      "source": [
        "# Delete the original Dataframe to save memory\n",
        "del raw_data_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "G8q3MvqjEAnP"
      },
      "source": [
        "# List of regular expressions to further filter the bodies of the comments;\n",
        "\n",
        "# Remove all links from the comments\n",
        "sdf.replace(r'https?://(www.)?[-a-zA-Z0-9@:%.+~#=]{1,256}.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9(_)@:%+.~#?&//=]*)','',regex=True, inplace = True)\n",
        "\n",
        "# Remove all user links/subreddit links\n",
        "sdf.replace(r'(/u/)?(r/)?(^)?(\\\\)?','',regex=True, inplace = True)\n",
        "\n",
        "# Replace '&gt;' and '&lt' with '<' and '>' respectively\n",
        "sdf.replace(r'(&gt;)','>',regex=True, inplace = True)\n",
        "sdf.replace(r'(&lt)','<',regex=True, inplace = True)\n",
        "\n",
        "# Replace '&amp' with an ampersand\n",
        "sdf.replace(r'(&amp;)','&',regex=True, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "f8-SmNcfEAnP"
      },
      "source": [
        "# Rename body column to response\n",
        "response_df = sdf.rename(columns={'body': 'response'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "suGiy-xJEAnP"
      },
      "source": [
        "# Show top ten rows\n",
        "response_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "tlQvcseIEAnP"
      },
      "source": [
        "# Create query-response pairs\n",
        "# Join the two tables to make a single one\n",
        "# Concatenate '<eos>' to the end, and '<sos>' to the start of the response and store each of them, in a different column\n",
        "sql_query = \" select inp.body\" \\\n",
        "            \"       , resp.response || ' <eos>'\" \\\n",
        "            \"       , '<sos> ' || resp.response\" \\\n",
        "            \" from response_df resp\" \\\n",
        "            \" left join sdf inp\" \\\n",
        "            \" on resp.parent_id = inp.name\" \\\n",
        "            \" where inp.body is not null and inp.body <> '' and resp.response <> ''\"\n",
        "# Can only be saved as sdf since this is how pandas works.\n",
        "sdf = ps.sqldf(sql_query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "3dgen4yrEAnQ"
      },
      "source": [
        "# Rename body to input, second column to output and third column to output_input\n",
        "xy_df = sdf.rename(columns={'body': 'input', \"resp.response || ' <eos>'\": 'output', \"'<sos> ' || resp.response\": 'output_input'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "suIX7mn-EAnQ"
      },
      "source": [
        "# Delete sdf to free up memory\n",
        "del sdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "VQirSiQeEAnQ"
      },
      "source": [
        "# show top ten rows of the new dataframe\n",
        "xy_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "CB3rFHF5EAnQ"
      },
      "source": [
        "# Export to a json file this is so we don't have to run all previous cells again\n",
        "xy_df.to_json('xy_data_2015_05.json', orient='records', lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "N2mTiy8LEAnQ"
      },
      "source": [
        "# Delete all to free memory\n",
        "del response_df\n",
        "del xy_df\n",
        "del sql_query"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "J49AOmJIEAnQ"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "Now that we have a dataset of usable message-response pairs, lets preprocess the data. The tokenizer encodes words into numbers,\n",
        "a seperate tokenizer is used for the the message data and the output data. Next, we padded the messages to have a fixed size for all of our messages.\n",
        "The `.json` file created in the previous section can be downloaded from this [google drive](https://drive.google.com/file/d/1J65cyCx6Zp1AgzTCGrB1Oqye9nkokmot/view?usp=sharing) link."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "zKcVf2L3EAnQ"
      },
      "source": [
        "# import statements\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "FiKgSWPDEAnQ"
      },
      "source": [
        "# Load data from the saved json file\n",
        "xy_data_df = pd.read_json(r'xy_data_2015_05.json', orient='records', lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "o-y-9mIwEAnQ"
      },
      "source": [
        "# for testing purposes we reduced the dataframe to the first 10000 comments\n",
        "xy_data_df = xy_data_df[0:10000]  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "8RTjQjSuEAnQ"
      },
      "source": [
        "# Check a random row from the dataframe\n",
        "print(xy_data_df['input'][172])\n",
        "print(xy_data_df['output'][172])\n",
        "print(xy_data_df['output_input'][172])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "1p4vtd-2EAnQ"
      },
      "source": [
        "# set max number of words recognized by the model\n",
        "MAX_NUM_WORDS = 5000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "39sJh3rCEAnQ"
      },
      "source": [
        "# Text from the input column is tokenized\n",
        "\n",
        "input_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "\n",
        "# Tokenizer is fitted\n",
        "input_tokenizer.fit_on_texts(xy_data_df['input'])\n",
        "\n",
        "# Sequences are generated from the text\n",
        "input_integer_seq = input_tokenizer.texts_to_sequences(xy_data_df['input'])\n",
        "\n",
        "# { word: index} dictionary of the input_tokenizer\n",
        "word2idx_inputs = input_tokenizer.word_index\n",
        "print('Total unique words in the input: %s' % len(word2idx_inputs))\n",
        "\n",
        "# Word count and max input sentence length are stored\n",
        "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
        "print(\"Length of longest sentence in input: %g\" % max_input_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "CmbzFRMrEAnQ"
      },
      "source": [
        "# Text from the output and output_input columns are tokenized\n",
        "# the regex given is the default filter minus the '<' and '>' symbols,\n",
        "# as these have been handled using SQL in the previous section\n",
        "output_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n)')\n",
        "\n",
        "# Tokenizer is fitted\n",
        "output_tokenizer.fit_on_texts(pd.concat([xy_data_df['output'], xy_data_df['output_input']]))\n",
        "\n",
        "# Sequences are generated from the text\n",
        "output_integer_seq = output_tokenizer.texts_to_sequences(xy_data_df['output'])\n",
        "output_input_integer_seq = output_tokenizer.texts_to_sequences(xy_data_df['output_input'])\n",
        "\n",
        "# { word: index} dictionary of the output_tokenizer\n",
        "word2idx_outputs = output_tokenizer.word_index\n",
        "print('Total unique words in the output: %s' % len(word2idx_outputs))\n",
        "\n",
        "# Word count and max output sentence length are stored\n",
        "num_words_output = len(word2idx_outputs) + 1\n",
        "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
        "print(\"Length of longest sentence in the output: %g\" % max_out_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "K53DZMppEAnQ"
      },
      "source": [
        "# input_integer_seq is padded which will be fed into the encoder\n",
        "# max_input_len stores the maximum output sentence length\n",
        "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
        "print(\"encoder_input_sequences.shape:\", encoder_input_sequences.shape)\n",
        "print(\"encoder_input_sequences[172]:\", encoder_input_sequences[172])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bJoPwJ_iEAnQ"
      },
      "source": [
        "# Example word indices from input_tokenizer\n",
        "print(word2idx_inputs[\"ill\"])\n",
        "print(word2idx_inputs[\"skins\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "f4nmYXa5EAnQ"
      },
      "source": [
        "# output_input_integer_seq is padded which will be fed into the decoder\n",
        "# max_out_len stores the maximum output sentence length\n",
        "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
        "print(\"decoder_input_sequences.shape:\", decoder_input_sequences.shape)\n",
        "print(\"decoder_input_sequences[172]:\", decoder_input_sequences[172])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "g8nl9BXQEAnQ"
      },
      "source": [
        "# Example word indices from output_tokenizer\n",
        "print(word2idx_outputs[\"<eos>\"])\n",
        "print(word2idx_outputs[\"not\"])\n",
        "print(word2idx_outputs[\"correctly\"])\n",
        "# print(word2idx_outputs[\"invisibility\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "0WGSUTl5EAnQ"
      },
      "source": [
        "The following two cells visualize the progress made up until this point"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "QbOZ1QpAEAnQ"
      },
      "source": [
        "subset_dict = {str(value): 0 for key, value in input_tokenizer.word_index.items()}\n",
        "input_sequences = []\n",
        "\n",
        "# The input and response sentences are tokenized \n",
        "# and the token occurrences are counted in subset_dict\n",
        "for line in xy_data_df.iterrows():\n",
        "\n",
        "    # Input tokenization\n",
        "    token_list = input_tokenizer.texts_to_sequences([line[1][0]])[0]\n",
        "\n",
        "    for token in token_list:\n",
        "        subset_dict[str(token)] += 1\n",
        "\n",
        "    # print('input')\n",
        "    # print(token_list)\n",
        "    # print(tokenizer.sequences_to_texts([token_list]))\n",
        "    # print()\n",
        "\n",
        "    # Response tokenization\n",
        "    token_list = input_tokenizer.texts_to_sequences([line[1][1]])[0]\n",
        "\n",
        "    # print('response')\n",
        "    # print(token_list)\n",
        "    # print(tokenizer.sequences_to_texts([token_list]))\n",
        "    # print()\n",
        "\n",
        "    for token in token_list:\n",
        "        subset_dict[str(token)] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "xuflUOPmEAnQ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# A sequence from 0 to 29 is created\n",
        "list_c = [i for i in range(30)]\n",
        "\n",
        "# The string value of the 30 most used tokens are retrieved\n",
        "example_seq = input_tokenizer.sequences_to_texts([list_c])[0]\n",
        "print(example_seq)\n",
        "\n",
        "# Turns the example_seq string into a list of words\n",
        "x = example_seq.split()\n",
        "\n",
        "# The 30 most popular words are plotted based on their occurrence\n",
        "plt.bar(x, list(subset_dict.values())[:len(x)], align = 'center')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "nCRQR6FeEAnQ"
      },
      "source": [
        "### Word embeddings\n",
        "\n",
        "This is where our work for the second milestone starts. We relied heavily on [this](https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/) guide on stackabuse. Although we made everal modifications to get it to work with our dataset.\n",
        "\n",
        "The following section embeds the words recognized by the model in a vector with 100 dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ZgT0Tz8PEAnQ"
      },
      "source": [
        "# import statements\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "HH-MEkw8EAnQ"
      },
      "source": [
        "# we used the pretrained vector embedding model GloVe\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "DbruJY6ZEAnQ"
      },
      "source": [
        "# unzip the downloaded file\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "MD1dEpqNEAnQ"
      },
      "source": [
        "#set embedding size\n",
        "EMBEDDING_SIZE = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "XlfnggUSEAnQ"
      },
      "source": [
        "# the embedding dictionary is a dictionary with the key being a word,\n",
        "# and the value being the corresponding 100d vector\n",
        "embeddings_dictionary = dict()\n",
        "\n",
        "# open the file containing the 100d vectors\n",
        "glove_file = open(r'glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "# iterate over the lines in the file\n",
        "for line in glove_file:\n",
        "    records = line.split()  # split along whitespaces\n",
        "    word = records[0]       # the word itself is the first element of the list\n",
        "    # the vector representation is the rest of the elements\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')  \n",
        "    embeddings_dictionary[word] = vector_dimensions  # insert word: vector representation into dictionary\n",
        "glove_file.close()  # close GloVe file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "LBw6dXW4EAnQ"
      },
      "source": [
        "# create the embedding matrix\n",
        "\n",
        "# limit the number of words understood by the model to MAX_NUM_WORDS\n",
        "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
        "# create embedding matrix filled with zeroes\n",
        "embedding_matrix = zeros((num_words, EMBEDDING_SIZE))\n",
        "\n",
        "# iterate over the first MAX_NUM_WORDS collected by the tokenizer\n",
        "for word, index in list(word2idx_inputs.items())[:num_words-1]:\n",
        "  # get embedding vector corresponding to the given word\n",
        "  embedding_vector = embeddings_dictionary.get(word)\n",
        "  # if embedding vector exists, the insert into relevant column of the mbedding matrix\n",
        "  # null vector by default\n",
        "  if embedding_vector is not None:\n",
        "      embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "a1vtxbduEAnQ"
      },
      "source": [
        "# some testing\n",
        "index = 4997"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "aym_AfDlEAnQ"
      },
      "source": [
        "# print last word\n",
        "print(list(word2idx_inputs.items())[index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "p4oNVENzEAnQ"
      },
      "source": [
        "# print embedding of word from the embedding dictionary\n",
        "print(embeddings_dictionary[list(word2idx_inputs.items())[index][0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "2iK64ssPEAnQ"
      },
      "source": [
        "# print embedding of word from the embedding matrix\n",
        "print(embedding_matrix[list(word2idx_inputs.items())[index][1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "t7DVCxUiEAnQ"
      },
      "source": [
        "### Model structure\n",
        "This section build the model and trains it based on the data compiled in the previous sections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "m3l7Jes8EAnQ"
      },
      "source": [
        "# import statements\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "qSKZRfliEAnQ"
      },
      "source": [
        "# create embedding layer from embedding matrix\n",
        "embedding_layer = Embedding(num_words, EMBEDDING_SIZE, weights=[embedding_matrix], input_length=max_input_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "VabwxLp_EAnQ"
      },
      "source": [
        "# get input sentences\n",
        "input_sentences = xy_data_df['input']\n",
        "\n",
        "# create null hypermatrix with dimensions:\n",
        "# number of input sentences\n",
        "# maximum word length of input sentences\n",
        "# number of words\n",
        "decoder_targets_one_hot = np.zeros((\n",
        "        len(input_sentences),\n",
        "        max_out_len,\n",
        "        num_words\n",
        "    ),\n",
        "    dtype='float32'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "qHRkSRZdEAnQ"
      },
      "source": [
        "# check shape\n",
        "decoder_targets_one_hot.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "gv84VrtpEAnQ"
      },
      "source": [
        "# pad output sequences to the same length,\n",
        "# namely to the maximum length of the output sequences\n",
        "decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "1CSpDHBrEAnQ"
      },
      "source": [
        "# fill the previously create null hypermatrix with one hot columns in the following fashion:\n",
        "# Insert value 1 into every r-th row of every c-th column of every m-th matrix where;\n",
        "# m is the index of the sentence in decoder_output_sequences: 1-st sentence -> m = 0, n-th sentence -> m = n-1\n",
        "# c is the place of the word in the sentence: 1-st word in sentence -> c = 0, n-th word in sentence -> c = n-1\n",
        "# r is the value given to the word by th output tokenizer: '<eos>' -> r = 1, 'not' -> r = 15\n",
        "\n",
        "for m, sequence in enumerate(decoder_output_sequences):\n",
        "    for c, r in enumerate(sequence):\n",
        "        decoder_targets_one_hot[m, c, r] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Pmt_oUXxEAnQ"
      },
      "source": [
        "# set number of LSTM nodes\n",
        "LSTM_NODES = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "1DYE_Zq_EAnQ"
      },
      "source": [
        "# import statements\n",
        "from tensorflow.keras.layers import LSTM, Input, Dense\n",
        "from tensorflow.keras import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "gLoIPcSSEAnQ"
      },
      "source": [
        "encoder_inputs_placeholder = Input(shape=(max_input_len,))\n",
        "x = embedding_layer(encoder_inputs_placeholder)\n",
        "encoder = LSTM(LSTM_NODES, return_state=True)\n",
        "\n",
        "encoder_outputs, h, c = encoder(x)\n",
        "encoder_states = [h, c]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Et9T87nSEAnR"
      },
      "source": [
        "decoder_inputs_placeholder = Input(shape=(max_out_len,))\n",
        "\n",
        "decoder_embedding = Embedding(num_words, LSTM_NODES)\n",
        "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
        "\n",
        "decoder_lstm = LSTM(LSTM_NODES, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "wIHEi64mEAnR"
      },
      "source": [
        "decoder_dense = Dense(num_words, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "lWtRZienEAnR"
      },
      "source": [
        "model = Model([encoder_inputs_placeholder,\n",
        "  decoder_inputs_placeholder], decoder_outputs)\n",
        "model.compile(\n",
        "    optimizer='rmsprop',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "5WbpsoQJEAnR"
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='model_plot4a.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "M3je1vilEAnS"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "RkiW6AZBEAnS"
      },
      "source": [
        "print(encoder_input_sequences.shape)\n",
        "print(decoder_input_sequences.shape)\n",
        "print(decoder_targets_one_hot.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "7go_L7giEAnS"
      },
      "source": [
        "r = model.fit(\n",
        "    [encoder_input_sequences, decoder_input_sequences],\n",
        "    decoder_targets_one_hot,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_split=0.2,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "WQuhIcazEAnS"
      },
      "source": [
        "### Encoder model structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ICZ4ZdTcEAnS"
      },
      "source": [
        "encoder_model = Model(encoder_inputs_placeholder, encoder_states)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "kz1PkhNaEAnS"
      },
      "source": [
        "decoder_state_input_h = Input(shape=(LSTM_NODES,))\n",
        "decoder_state_input_c = Input(shape=(LSTM_NODES,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "29U4BemMEAnS"
      },
      "source": [
        "decoder_inputs_single = Input(shape=(1,))\n",
        "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "FyQL446gEAnT"
      },
      "source": [
        "decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "uVuty0oXEAnT"
      },
      "source": [
        "decoder_states = [h, c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Xc_lNqdnEAnT"
      },
      "source": [
        "decoder_model = Model(\n",
        "    [decoder_inputs_single] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "nkW54arrEAnT"
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(decoder_model, to_file='model_plot_dec.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "FtYCvNOkEAnT"
      },
      "source": [
        "idx2word_input = {v:k for k, v in word2idx_inputs.items()}\n",
        "idx2word_target = {v:k for k, v in word2idx_outputs.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bDKACKT3EAnT"
      },
      "source": [
        "def translate_sentence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "    eos = word2idx_outputs['<eos>']\n",
        "    output_sentence = []\n",
        "\n",
        "    for _ in range(max_out_len):\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        idx = np.argmax(output_tokens[0, 0, :])\n",
        "\n",
        "        if eos == idx:\n",
        "            break\n",
        "\n",
        "        word = ''\n",
        "\n",
        "        if idx > 0:\n",
        "            word = idx2word_target[idx]\n",
        "            output_sentence.append(word)\n",
        "\n",
        "        target_seq[0, 0] = idx\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return ' '.join(output_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "xqd4EMvzEAnU"
      },
      "source": [
        "### Evaluation\n",
        "Evaluate a few input sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "kAm445u9EAnU"
      },
      "source": [
        "i = np.random.choice(len(input_sentences))\n",
        "input_seq = encoder_input_sequences[i:i+1]\n",
        "translation = translate_sentence(input_seq)\n",
        "print('-')\n",
        "print('Input:', input_sentences[i])\n",
        "print('Response:', translation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "HQNtm_-mEAnU"
      },
      "source": [
        "### Save model\n",
        "This section saves and download the model. The saved model was too large for github, it is available as a [google drive](https://drive.google.com/file/d/1mpYifGZ_TLrer6ZgRtNKO25xSuMOPi1u/view?usp=sharing) link."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "3gMZbdUfEAnU"
      },
      "source": [
        "# save model\n",
        "model.save('model_1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "oOfkNCgAEAnU"
      },
      "source": [
        "# zip saved model\n",
        "!zip -r /content/model_1.zip /content/model_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "r6lg90T7EAnU"
      },
      "source": [
        "# download saved model\n",
        "from google.colab import files\n",
        "files.download('model_1.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qbzHxJUEFdd"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "This model still has a way to go, but with a single training cycle the results were positive. The future goal is to create a better bot via training and trying to make a better model in general. We expect to arrive at a somewhat sensible model (as sensible as a model trained on reddit comments can be).\n",
        "The desired result would be a model which can react to inputs properly (in a way that makes sense in the given context). \n",
        "\n",
        "We will try to mitigate some of the problems we found in the answer the bot gave, such as words repeating and the bot giving an irrelevant answer. This can be achieved by using more data, having more training cycles, filtering the training data better (this is a hard thing to do, since there are so many comments that this needs to be automated, and creating an algorithm which can filter good data from bad data is hard) and trying different models.\n",
        "\n",
        "The end result won't be quantifiable by a computer, we need to label good and bad outcomes.\n"
      ]
    }
  ]
}